{"paragraphs":[{"text":"%md # Forest Fire Prediction through KMeans Clustering\n<img src=\"https://surveymonkey-assets.s3.amazonaws.com/survey/121135814/6a48257c-8996-4aa6-ba56-6b1e373385c3.png\" width=100 hspace=\"20\" style=\"float: right;\">\nThe United States Forest Service provides datasets that describe forest fires that have occurred in Canada and the United States since year 2000. We can  predict where forest fires are prone to occur by partitioning the locations of past burns into clusters whose centroids can be used to optimally place heavy fire fighting equipment as near as possible to where fires are likely to occur.\n\nDataset:\nhttps://fsapps.nwcg.gov/gisdata.php","user":"anonymous","dateUpdated":"2019-02-13T21:37:11+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Forest Fire Prediction through KMeans Clustering</h1>\n<img src=\"https://surveymonkey-assets.s3.amazonaws.com/survey/121135814/6a48257c-8996-4aa6-ba56-6b1e373385c3.png\" width=100 hspace=\"20\" style=\"float: right;\">\n<p>The United States Forest Service provides datasets that describe forest fires that have occurred in Canada and the United States since year 2000. We can predict where forest fires are prone to occur by partitioning the locations of past burns into clusters whose centroids can be used to optimally place heavy fire fighting equipment as near as possible to where fires are likely to occur.</p>\n<p>Dataset:<br/><a href=\"https://fsapps.nwcg.gov/gisdata.php\">https://fsapps.nwcg.gov/gisdata.php</a></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1550093831244_-1321439742","id":"20171024-051623_427668146","dateCreated":"2019-02-13T21:37:11+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:238"},{"title":"Demo Setup Instructions","text":"%md ## Preliminary Setup\nHere are all the things that need to be in place for this notebook to run:\n\nDownload the MapR Sandbox for Hadoop\nImport into VirutalBox\nSetup networking for the VM\nIncrease memory to 8GB.\nStart the VM\n\nSSH to the VM\n\nWe need a license in order to do snapshots, so here's how to create a license:\nLog onto mapr.com/user\nCreate a new license:\nRun this to get the cluster ID:\n\n  maprcli license showid\n\nRun this to get the cluster name:\n\n  maprcli license list\n\nGenerate the license then click View Key\n\nCopy and Paste the key to license.txt on the mapr node\n\nRun this command to add the license:\n  maprcli license add -license license.txt -is_file true\n\nput user mapr in sudoers\n\n  su\n  chmod 640 /etc/sudoers && echo \"mapr        ALL=(ALL)       NOPASSWD: ALL\" >> /etc/sudoers && chmod 440 /etc/sudoers\n\nInstall python dependencies for the notebook:\n\n  sudo yum install python-pip -y\n  sudo pip install dbfpy requests pandas bs4 websocket-client\n\n  wget https://github.com/joewalnes/websocketd/releases/download/v0.3.0/websocketd.0.3.0.i386.rpm\n  sudo rpm -ivh websocketd.0.3.0.i386.rpm\n\nCopy the following files to /home/mapr/:\n\n```\nml_input_stream.sh\nml_output_stream.sh\nmapr-sparkml-streaming-fires-1.0-jar-with-dependencies.jar\n```\n\nDownload zeppelin. There's not enough disk space in the VM to install zeppelin with all the interpretters, so download this one, which just includes the Spark interpretter:\n\n  wget http://apache.cs.utah.edu/zeppelin/zeppelin-0.8.0/zeppelin-0.8.0-bin-netinst.tgz\n  tar -xzvf zeppelin-0.8.0-bin-netinst.tgz\n  cd zeppelin-0.8.0-bin-netinst/\n\nInstall the markdown interpretter:\n   \n   ./bin/install-interpreter.sh --name md\n\nInstall the shell interpretter:\n   ./bin/install-interpreter.sh --name shell \n\nAdd this to the artiface dependencies in the zeppelin config screen:\n\n  org.apache.zeppelin:zeppelin-shell:0.8.0\n\n  or whatever is output by this command:\n    grep shell conf/interpreter-list | awk '{print $2}'\n\nInstall the angular interpretter:\n  ./bin/install-interpreter.sh --name angular\n\n  For some reason, this new interpretter won't show up in zeppelin's interpretter config screen, so go to that screen, click the create button, secify \"angular\" as the interpretter name and interpreter group, then put this as the artifact and click save and restart:\n \n    org.apache.zeppelin:zeppelin-angular:0.8.0\n  \n  or whatever is output by this command:\n    grep angular conf/interpreter-list | awk '{print $2}'\n\nSetup Spark home property:\n\n  echo \"export SPARK_HOME=/opt/mapr/spark/spark-2.2.1/\" >> conf/zeppelin-env.sh\n\nStart Zeppelin\n  bin/zeppelin-daemon.sh start\n\nopen http://10.0.0.101:8080\n\nThen open your notebook that uses angular, click the gear icon in the top right, and attach the angular interpretter to the notebook and click save.\n\nDo the same for the markdown (md) interpretter.\n\nAdd classpath to shell interetter config:\n\"groupArtifactVersion\": \"org.apache.zeppelin:zeppelin-shell:0.8.0\",\n","user":"anonymous","dateUpdated":"2019-02-13T21:37:11+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Preliminary Setup</h2>\n<p>Here are all the things that need to be in place for this notebook to run:</p>\n<p>Download the MapR Sandbox for Hadoop<br/>Import into VirutalBox<br/>Setup networking for the VM<br/>Increase memory to 8GB.<br/>Start the VM</p>\n<p>SSH to the VM</p>\n<p>We need a license in order to do snapshots, so here&rsquo;s how to create a license:<br/>Log onto mapr.com/user<br/>Create a new license:<br/>Run this to get the cluster ID:</p>\n<p>maprcli license showid</p>\n<p>Run this to get the cluster name:</p>\n<p>maprcli license list</p>\n<p>Generate the license then click View Key</p>\n<p>Copy and Paste the key to license.txt on the mapr node</p>\n<p>Run this command to add the license:<br/> maprcli license add -license license.txt -is_file true</p>\n<p>put user mapr in sudoers</p>\n<p>su<br/> chmod 640 /etc/sudoers &amp;&amp; echo &ldquo;mapr ALL=(ALL) NOPASSWD: ALL&rdquo; &gt;&gt; /etc/sudoers &amp;&amp; chmod 440 /etc/sudoers</p>\n<p>Install python dependencies for the notebook:</p>\n<p>sudo yum install python-pip -y<br/> sudo pip install dbfpy requests pandas bs4 websocket-client</p>\n<p>wget <a href=\"https://github.com/joewalnes/websocketd/releases/download/v0.3.0/websocketd.0.3.0.i386.rpm\">https://github.com/joewalnes/websocketd/releases/download/v0.3.0/websocketd.0.3.0.i386.rpm</a><br/> sudo rpm -ivh websocketd.0.3.0.i386.rpm</p>\n<p>Copy the following files to /home/mapr/:</p>\n<pre><code>ml_input_stream.sh\nml_output_stream.sh\nmapr-sparkml-streaming-fires-1.0-jar-with-dependencies.jar\n</code></pre>\n<p>Download zeppelin. There&rsquo;s not enough disk space in the VM to install zeppelin with all the interpretters, so download this one, which just includes the Spark interpretter:</p>\n<p>wget <a href=\"http://apache.cs.utah.edu/zeppelin/zeppelin-0.8.0/zeppelin-0.8.0-bin-netinst.tgz\">http://apache.cs.utah.edu/zeppelin/zeppelin-0.8.0/zeppelin-0.8.0-bin-netinst.tgz</a><br/> tar -xzvf zeppelin-0.8.0-bin-netinst.tgz<br/> cd zeppelin-0.8.0-bin-netinst/</p>\n<p>Install the markdown interpretter:</p>\n<p>./bin/install-interpreter.sh &ndash;name md</p>\n<p>Install the shell interpretter:<br/> ./bin/install-interpreter.sh &ndash;name shell </p>\n<p>Add this to the artiface dependencies in the zeppelin config screen:</p>\n<p>org.apache.zeppelin:zeppelin-shell:0.8.0</p>\n<p>or whatever is output by this command:<br/> grep shell conf/interpreter-list | awk &lsquo;{print $2}&rsquo;</p>\n<p>Install the angular interpretter:<br/> ./bin/install-interpreter.sh &ndash;name angular</p>\n<p>For some reason, this new interpretter won&rsquo;t show up in zeppelin&rsquo;s interpretter config screen, so go to that screen, click the create button, secify &ldquo;angular&rdquo; as the interpretter name and interpreter group, then put this as the artifact and click save and restart:</p>\n<pre><code>org.apache.zeppelin:zeppelin-angular:0.8.0\n</code></pre>\n<p>or whatever is output by this command:<br/> grep angular conf/interpreter-list | awk &lsquo;{print $2}&rsquo;</p>\n<p>Setup Spark home property:</p>\n<p>echo &ldquo;export SPARK_HOME=/opt/mapr/spark/spark-2.2.1/&rdquo; &gt;&gt; conf/zeppelin-env.sh</p>\n<p>Start Zeppelin<br/> bin/zeppelin-daemon.sh start</p>\n<p>open <a href=\"http://10.0.0.101:8080\">http://10.0.0.101:8080</a></p>\n<p>Then open your notebook that uses angular, click the gear icon in the top right, and attach the angular interpretter to the notebook and click save.</p>\n<p>Do the same for the markdown (md) interpretter.</p>\n<p>Add classpath to shell interetter config:<br/>&ldquo;groupArtifactVersion&rdquo;: &ldquo;org.apache.zeppelin:zeppelin-shell:0.8.0&rdquo;,</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1550093831250_-1282403698","id":"20181002-101723_678609542","dateCreated":"2019-02-13T21:37:11+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:239"},{"title":"Download Raw Data ","text":"%sh\nmkdir -p /mapr/demo.mapr.com/user/mapr/data/fires\ncd /mapr/demo.mapr.com/user/mapr/data/fires\ncurl -s --remote-name https://fsapps.nwcg.gov/afm/data/fireptdata/modis_fire_2016_365_conus_shapefile.zip\ncurl -s --remote-name https://fsapps.nwcg.gov/afm/data/fireptdata/modis_fire_2015_365_conus_shapefile.zip\ncurl -s --remote-name https://fsapps.nwcg.gov/afm/data/fireptdata/modis_fire_2014_365_conus_shapefile.zip\ncurl -s --remote-name https://fsapps.nwcg.gov/afm/data/fireptdata/modis_fire_2013_365_conus_shapefile.zip\ncurl -s --remote-name https://fsapps.nwcg.gov/afm/data/fireptdata/modis_fire_2012_366_conus_shapefile.zip\ncurl -s --remote-name https://fsapps.nwcg.gov/afm/data/fireptdata/modis_fire_2011_365_conus_shapefile.zip\ncurl -s --remote-name https://fsapps.nwcg.gov/afm/data/fireptdata/modis_fire_2010_365_conus_shapefile.zip\ncurl -s --remote-name https://fsapps.nwcg.gov/afm/data/fireptdata/modis_fire_2009_365_conus_shapefile.zip\ncurl -s --remote-name https://fsapps.nwcg.gov/afm/data/fireptdata/mcd14ml_2008_005_01_conus_shp.zip\ncurl -s --remote-name https://fsapps.nwcg.gov/afm/data/fireptdata/mcd14ml_2007_005_01_conus_shp.zip\ncurl -s --remote-name https://fsapps.nwcg.gov/afm/data/fireptdata/mcd14ml_2006_005_01_conus_shp.zip\ncurl -s --remote-name https://fsapps.nwcg.gov/afm/data/fireptdata/mcd14ml_2005_005_01_conus_shp.zip\ncurl -s --remote-name https://fsapps.nwcg.gov/afm/data/fireptdata/mcd14ml_2004_005_01_conus_shp.zip\ncurl -s --remote-name https://fsapps.nwcg.gov/afm/data/fireptdata/mcd14ml_2003_005_01_conus_shp.zip\ncurl -s --remote-name https://fsapps.nwcg.gov/afm/data/fireptdata/mcd14ml_2002_005_01_conus_shp.zip\ncurl -s --remote-name https://fsapps.nwcg.gov/afm/data/fireptdata/mcd14ml_2001_005_01_conus_shp.zip\nfind modis*.zip | xargs -I {} unzip {} modis*.dbf\nfind mcd*.zip | xargs -I {} unzip {} mcd*.dbf","user":"anonymous","dateUpdated":"2019-02-13T21:37:31+0000","config":{"tableHide":false,"editorSetting":{"language":"sh","editOnDblClick":false,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","editorHide":false,"fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Archive:  modis_fire_2009_365_conus_shapefile.zip\n  inflating: modis_fire_2009_365_conus.dbf  \nArchive:  modis_fire_2010_365_conus_shapefile.zip\n  inflating: modis_fire_2010_365_conus.dbf  \nArchive:  modis_fire_2011_365_conus_shapefile.zip\n  inflating: modis_fire_2011_365_conus.dbf  \nArchive:  modis_fire_2012_366_conus_shapefile.zip\n  inflating: modis_fire_2012_366_conus.dbf  \nArchive:  modis_fire_2013_365_conus_shapefile.zip\n  inflating: modis_fire_2013_365_conus.dbf  \nArchive:  modis_fire_2014_365_conus_shapefile.zip\n  inflating: modis_fire_2014_365_conus.dbf  \nArchive:  modis_fire_2015_365_conus_shapefile.zip\n  inflating: modis_fire_2015_365_conus.dbf  \nArchive:  modis_fire_2016_365_conus_shapefile.zip\n  inflating: modis_fire_2016_365_conus.dbf  \nArchive:  mcd14ml_2001_005_01_conus_shp.zip\n  inflating: mcd14ml_2001_005_01_conus.dbf  \nArchive:  mcd14ml_2002_005_01_conus_shp.zip\n  inflating: mcd14ml_2002_005_01_conus.dbf  \nArchive:  mcd14ml_2003_005_01_conus_shp.zip\n  inflating: mcd14ml_2003_005_01_conus.dbf  \nArchive:  mcd14ml_2004_005_01_conus_shp.zip\n  inflating: mcd14ml_2004_005_01_conus.dbf  \nArchive:  mcd14ml_2005_005_01_conus_shp.zip\n  inflating: mcd14ml_2005_005_01_conus.dbf  \nArchive:  mcd14ml_2006_005_01_conus_shp.zip\n  inflating: mcd14ml_2006_005_01_conus.dbf  \nArchive:  mcd14ml_2007_005_01_conus_shp.zip\n  inflating: mcd14ml_2007_005_01_conus.dbf  \nArchive:  mcd14ml_2008_005_01_conus_shp.zip\n  inflating: mcd14ml_2008_005_01_conus.dbf  \n"}]},"apps":[],"jobName":"paragraph_1550093831251_-1008127911","id":"20171024-201248_1170429857","dateCreated":"2019-02-13T21:37:11+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:240","dateFinished":"2019-02-13T21:38:20+0000","dateStarted":"2019-02-13T21:37:32+0000"},{"title":"Backup original dataset","text":"%sh\nmaprcli volume snapshot create -cluster demo.mapr.com -snapshotname USFS_Experiment-`date +%Y%m%d%H%M%S` -volume users\nls -la /mapr/demo.mapr.com/user/.snapshot/\n\n","user":"anonymous","dateUpdated":"2019-02-13T21:38:21+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"total 2\ndrwxr-xr-x 3 root root 1 Feb 13 21:38 .\ndrwxr-xr-x 4 mapr mapr 2 Dec 18 18:51 ..\ndrwxr-xr-x 4 mapr mapr 2 Dec 18 18:51 USFS_Experiment-20190213213821\n"}]},"apps":[],"jobName":"paragraph_1550093831251_31526888","id":"20171110-052214_828862000","dateCreated":"2019-02-13T21:37:11+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:241","dateFinished":"2019-02-13T21:38:25+0000","dateStarted":"2019-02-13T21:38:21+0000"},{"title":"Convert shapefiles to CSVs","text":"%python\nimport csv\nfrom dbfpy import dbf\nimport os\nimport sys\nDATADIR='/mapr/demo.mapr.com/user/mapr/data/fires/'\n\nfor filename in os.listdir(DATADIR):\n\n    if filename.endswith('.dbf'):\n        print \"Converting %s to csv\" % filename\n        csv_fn = DATADIR+filename[:-4]+ \".csv\"\n        with open(csv_fn,'wb') as csvfile:\n            in_db = dbf.Dbf(DATADIR+filename)\n            out_csv = csv.writer(csvfile)\n            names = []\n            for field in in_db.header.fields:\n                names.append(field.name)\n            out_csv.writerow(names)\n            for rec in in_db:\n                out_csv.writerow(rec.fieldData)\n            in_db.close()\n            print \"Done...\"\n\n","user":"anonymous","dateUpdated":"2019-02-13T21:38:30+0000","config":{"tableHide":false,"editorSetting":{"language":"python","editOnDblClick":false,"completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Converting mcd14ml_2002_005_01_conus.dbf to csv\nDone...\nConverting modis_fire_2009_365_conus.dbf to csv\nDone...\nConverting modis_fire_2013_365_conus.dbf to csv\nDone...\nConverting mcd14ml_2005_005_01_conus.dbf to csv\nDone...\nConverting modis_fire_2016_365_conus.dbf to csv\nDone...\nConverting modis_fire_2015_365_conus.dbf to csv\nDone...\nConverting mcd14ml_2004_005_01_conus.dbf to csv\nDone...\nConverting modis_fire_2011_365_conus.dbf to csv\nDone...\nConverting mcd14ml_2001_005_01_conus.dbf to csv\nDone...\nConverting modis_fire_2012_366_conus.dbf to csv\nDone...\nConverting mcd14ml_2003_005_01_conus.dbf to csv\nDone...\nConverting modis_fire_2014_365_conus.dbf to csv\nDone...\nConverting mcd14ml_2007_005_01_conus.dbf to csv\nDone...\nConverting mcd14ml_2008_005_01_conus.dbf to csv\nDone...\nConverting mcd14ml_2006_005_01_conus.dbf to csv\nDone...\nConverting modis_fire_2010_365_conus.dbf to csv\nDone...\n"}]},"apps":[],"jobName":"paragraph_1550093831252_-234383203","id":"20171024-202149_2037979424","dateCreated":"2019-02-13T21:37:11+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:242","dateFinished":"2019-02-13T21:41:57+0000","dateStarted":"2019-02-13T21:38:30+0000"},{"title":"Import Spark ML Libraries","text":"import org.apache.spark._\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.sql.types._\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql._\nimport org.apache.spark._\nimport org.apache.spark.ml.feature.StringIndexer\nimport org.apache.spark.ml.feature.VectorAssembler\nimport org.apache.spark.ml.clustering.KMeans\nimport org.apache.spark.ml.clustering.KMeansModel\nimport org.apache.spark.mllib.linalg.Vectors","user":"anonymous","dateUpdated":"2019-02-13T21:39:08+0000","config":{"tableHide":true,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"fontSize":9,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark._\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.sql.types._\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql._\nimport org.apache.spark._\nimport org.apache.spark.ml.feature.StringIndexer\nimport org.apache.spark.ml.feature.VectorAssembler\nimport org.apache.spark.ml.clustering.KMeans\nimport org.apache.spark.ml.clustering.KMeansModel\nimport org.apache.spark.mllib.linalg.Vectors\n"}]},"apps":[],"jobName":"paragraph_1550093831252_-933374690","id":"20161030-025214_1655763979","dateCreated":"2019-02-13T21:37:11+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:243","dateFinished":"2019-02-13T21:39:08+0000","dateStarted":"2019-02-13T21:39:08+0000"},{"title":"Define schema for datasets since 2009","text":"import sqlContext.implicits._\nimport sqlContext._\nval modis_schema = StructType(Array(\n  StructField(\"area\", DoubleType, true),\n  StructField(\"perimeter\", DoubleType, true),\n  StructField(\"firenum\", IntegerType, true),      \n  StructField(\"fire_id\", IntegerType, true),      \n  StructField(\"latitude\", DoubleType, true),\n  StructField(\"longitude\", DoubleType, true),\n  StructField(\"date\", TimestampType, true),\n  StructField(\"julian\", IntegerType, true),\n  StructField(\"gmt\", IntegerType, true),\n  StructField(\"temp\", DoubleType, true),     \n  StructField(\"spix\", DoubleType, true),      \n  StructField(\"tpix\", DoubleType, true),            \n  StructField(\"src\", StringType, true),\n  StructField(\"sat_src\", StringType, true),      \n  StructField(\"conf\", IntegerType, true),\n  StructField(\"frp\", DoubleType, true)\n))","user":"anonymous","dateUpdated":"2019-02-13T21:39:10+0000","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import sqlContext.implicits._\nimport sqlContext._\nmodis_schema: org.apache.spark.sql.types.StructType = StructType(StructField(area,DoubleType,true), StructField(perimeter,DoubleType,true), StructField(firenum,IntegerType,true), StructField(fire_id,IntegerType,true), StructField(latitude,DoubleType,true), StructField(longitude,DoubleType,true), StructField(date,TimestampType,true), StructField(julian,IntegerType,true), StructField(gmt,IntegerType,true), StructField(temp,DoubleType,true), StructField(spix,DoubleType,true), StructField(tpix,DoubleType,true), StructField(src,StringType,true), StructField(sat_src,StringType,true), StructField(conf,IntegerType,true), StructField(frp,DoubleType,true))\n"}]},"apps":[],"jobName":"paragraph_1550093831253_-298536512","id":"20161030-030543_519944270","dateCreated":"2019-02-13T21:37:11+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:244","dateFinished":"2019-02-13T21:39:12+0000","dateStarted":"2019-02-13T21:39:11+0000"},{"title":"Define schema for datasets before 2009","text":"import sqlContext.implicits._\nimport sqlContext._\nval mcd14ml_schema = StructType(Array(\n  StructField(\"area\", DoubleType, true),\n  StructField(\"perimeter\", DoubleType, true),\n  StructField(\"mcd14ml_\", IntegerType, true),      \n  StructField(\"latitude\", DoubleType, true),      \n  StructField(\"longitude\", DoubleType, true),\n  StructField(\"t21\", DoubleType, true),\n  StructField(\"t31\", DoubleType, true),\n  StructField(\"spix\", DoubleType, true),\n  StructField(\"tpix\", DoubleType, true),\n  StructField(\"date\", TimestampType, true),     \n  StructField(\"jdate\", IntegerType, true),      \n  StructField(\"utc\", IntegerType, true),            \n  StructField(\"satellite\", StringType, true),\n  StructField(\"frp\", DoubleType, true),      \n  StructField(\"confidence\", IntegerType, true)\n))","user":"anonymous","dateUpdated":"2019-02-13T21:39:16+0000","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"fontSize":9,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import sqlContext.implicits._\nimport sqlContext._\nmcd14ml_schema: org.apache.spark.sql.types.StructType = StructType(StructField(area,DoubleType,true), StructField(perimeter,DoubleType,true), StructField(mcd14ml_,IntegerType,true), StructField(latitude,DoubleType,true), StructField(longitude,DoubleType,true), StructField(t21,DoubleType,true), StructField(t31,DoubleType,true), StructField(spix,DoubleType,true), StructField(tpix,DoubleType,true), StructField(date,TimestampType,true), StructField(jdate,IntegerType,true), StructField(utc,IntegerType,true), StructField(satellite,StringType,true), StructField(frp,DoubleType,true), StructField(confidence,IntegerType,true))\n"}]},"apps":[],"jobName":"paragraph_1550093831253_1693102754","id":"20171026-031641_291214581","dateCreated":"2019-02-13T21:37:11+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:245","dateFinished":"2019-02-13T21:39:18+0000","dateStarted":"2019-02-13T21:39:16+0000"},{"title":"Load Raw Data","text":"// Load datasets containing years 2009-2016\nval df_modis_all = sqlContext.read.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").schema(modis_schema).load(\"/user/mapr/data/fires/modis*.csv\")\n// Include only fires with coordinates in Cascadia\nval df_modis = df_modis_all.filter($\"latitude\" > 42).filter($\"latitude\" < 50).filter($\"longitude\" > -124).filter($\"longitude\" < -110)\n// Load datasets containing years 2000-2008\nval df_mcd14ml_all = sqlContext.read.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").schema(modis_schema).load(\"/user/mapr/data/fires/mcd14ml*.csv\")\n// Include only fires with coordinates in Cascadia\nval df_mcd14ml = df_mcd14ml_all.filter($\"latitude\" > 42).filter($\"latitude\" < 50).filter($\"longitude\" > -124).filter($\"longitude\" < -110)\n// Join both datasets\nval df = df_modis.union(df_mcd14ml).select($\"latitude\", $\"longitude\")","user":"anonymous","dateUpdated":"2019-02-13T21:39:20+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"df_modis_all: org.apache.spark.sql.DataFrame = [area: double, perimeter: double ... 14 more fields]\ndf_modis: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [area: double, perimeter: double ... 14 more fields]\ndf_mcd14ml_all: org.apache.spark.sql.DataFrame = [area: double, perimeter: double ... 14 more fields]\ndf_mcd14ml: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [area: double, perimeter: double ... 14 more fields]\ndf: org.apache.spark.sql.DataFrame = [latitude: double, longitude: double]\n"}]},"apps":[],"jobName":"paragraph_1550093831254_1311608418","id":"20161030-030618_394385178","dateCreated":"2019-02-13T21:37:11+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:246","dateFinished":"2019-02-13T21:39:24+0000","dateStarted":"2019-02-13T21:39:20+0000"},{"title":"What does this data look like, anyway?","text":"df_modis.show(10)\ndf.count()","user":"anonymous","dateUpdated":"2019-02-13T21:39:23+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+----+---------+-------+-------+--------+---------+-------------------+------+----+-----+----+----+----+-------+----+-------+\n|area|perimeter|firenum|fire_id|latitude|longitude|               date|julian| gmt| temp|spix|tpix| src|sat_src|conf|    frp|\n+----+---------+-------+-------+--------+---------+-------------------+------+----+-----+----+----+----+-------+----+-------+\n| 0.0|      0.0|     60| 327374|  49.441| -113.583|2009-11-01 00:00:00|   305|1011|307.0| 1.5| 1.2|ssec|      A|  50|   35.6|\n| 0.0|      0.0|     61| 327373|  49.441| -113.583|2009-11-01 00:00:00|   305|1010|307.0| 1.4| 1.2|gsfc|      A|  50|-9999.0|\n| 0.0|      0.0|     62| 327372|  49.441| -113.583|2009-11-01 00:00:00|   305|1010|307.0| 1.5| 1.2|rsac|      A|  50|   35.6|\n| 0.0|      0.0|    137| 328412|  49.419|  -118.97|2009-11-03 00:00:00|   307|2103|357.8| 1.3| 1.1|rsac|      A|  99|  133.9|\n| 0.0|      0.0|    138| 328411|  49.417|  -118.97|2009-11-03 00:00:00|   307|2110|357.8| 1.2| 1.1|gsfc|      A|  99|-9999.0|\n| 0.0|      0.0|    163| 107061|  49.404| -112.861|2009-04-17 00:00:00|   107|2025|311.1| 1.0| 1.0|gsfc|      A|  46|-9999.0|\n| 0.0|      0.0|    164| 107060|  49.404|  -112.86|2009-04-17 00:00:00|   107|2025|311.1| 1.1| 1.0| uaf|      A|  46|    6.2|\n| 0.0|      0.0|    165| 107059|  49.404|  -112.86|2009-04-17 00:00:00|   107|2018|311.1| 1.1| 1.0|ssec|      A|  46|    6.2|\n| 0.0|      0.0|    267|  59172|  49.374| -120.151|2009-03-22 00:00:00|    81|1015|310.3| 1.0| 1.0|gsfc|      A|  70|-9999.0|\n| 0.0|      0.0|    283| 321479|  49.368| -116.889|2009-10-15 00:00:00|   288|2040|332.9| 1.0| 1.0|gsfc|      A|  74|-9999.0|\n+----+---------+-------+-------+--------+---------+-------------------+------+----+-----+----+----+----+-------+----+-------+\nonly showing top 10 rows\n\nres2: Long = 73413\n"}]},"apps":[],"jobName":"paragraph_1550093831254_825975469","id":"20171026-045121_278676373","dateCreated":"2019-02-13T21:37:11+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:247","dateFinished":"2019-02-13T21:39:32+0000","dateStarted":"2019-02-13T21:39:23+0000","runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://8e7f2ab48c5c:4040/jobs/job?id=0","http://8e7f2ab48c5c:4040/jobs/job?id=1"],"interpreterSettingId":"spark"}}},{"title":"Train the KMeans model for 100 clusters","text":"val featureCols = Array(\"latitude\", \"longitude\")\nval assembler = new VectorAssembler().setInputCols(featureCols).setOutputCol(\"features\")\nval df2 = assembler.transform(df)\nval Array(trainingData, testData) = df2.randomSplit(Array(0.7, 0.3), 5043)\n\nval kmeans = new KMeans().setK(100).setFeaturesCol(\"features\").setMaxIter(4)\nval model = kmeans.fit(trainingData)\nprintln(\"Final Centers: \")\nmodel.clusterCenters.foreach(println)\n","user":"anonymous","dateUpdated":"2019-02-13T21:45:08+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Final Centers: \n[42.2905841346154,-120.96012019230768]\n[43.55954278606951,-114.54436815920418]\n[44.438264084507026,-119.6764084507042]\n[48.6438845654993,-115.15791309987024]\n[48.059677419354855,-116.59163636363634]\n[47.44492971887547,-115.03910642570277]\n[42.511442224153,-111.79813987836673]\n[47.942178217821784,-118.29569306930695]\n[42.26050684931508,-123.00764840182653]\n[42.97198029197082,-120.97199635036488]\n[45.7000360551431,-114.72772799575822]\n[47.546408695652175,-123.43842608695651]\n[47.66352941176471,-120.85551336898396]\n[45.857354166666674,-119.61592916666662]\n[45.92263636363637,-111.57022727272725]\n[46.34471299638989,-117.59815884476535]\n[48.20920614035085,-113.85592543859651]\n[46.48828151260502,-121.15913025210087]\n[49.13458389261745,-119.0663288590604]\n[44.10491489361703,-116.92098936170208]\n[45.0788940397351,-120.03639735099337]\n[42.724822857142854,-110.57778857142856]\n[42.538338709677426,-116.9060161290323]\n[46.64910234541577,-119.62101492537317]\n[44.40529087452472,-121.59957034220533]\n[44.72829702970297,-122.6448861386139]\n[46.99610801393729,-115.86469860627173]\n[48.941869047619036,-122.73575000000001]\n[48.74797894736843,-111.93202105263158]\n[47.66685915492958,-111.8605892018779]\n[42.236714285714285,-122.14154679802958]\n[46.64848275862067,-122.36135249042145]\n[44.931431742508366,-118.66029744728063]\n[43.17843243243246,-117.77659459459466]\n[46.382948113207554,-112.98791981132072]\n[43.68683333333333,-111.80408928571428]\n[46.67498542805104,-114.24993624772301]\n[47.74252237710925,-113.08680997798955]\n[46.23302015503874,-114.9433472868217]\n[45.94942,-116.79114666666666]\n[44.367672514619876,-118.26486257309942]\n[46.026855491329535,-120.57466589595387]\n[43.10431664470798,-122.66535090030756]\n[43.45886896551716,-115.521806239737]\n[43.95195321637426,-115.98867836257308]\n[44.66996907216494,-117.05801030927833]\n[43.78664414414416,-110.37222972972977]\n[43.67527463312369,-123.45175890985323]\n[48.3369623762376,-119.73696237623759]\n[43.516719640179936,-122.22657571214393]\n[43.63235174603175,-115.0688330158728]\n[42.7958995983936,-123.60537014725584]\n[42.48589025069638,-123.82513593314762]\n[44.92372903225807,-116.13591612903225]\n[48.63020512820513,-110.55042307692312]\n[45.60761246612463,-121.34604065040658]\n[49.114452554744524,-117.90402189781024]\n[48.38373333333335,-118.97791111111113]\n[44.87310688836102,-123.53880522565318]\n[42.61568017057569,-114.79116204690823]\n[43.96750485436893,-119.31991262135922]\n[47.670715686274534,-110.78963235294114]\n[46.25562428571428,-118.11347428571429]\n[42.90540687679081,-118.54564469914044]\n[45.95004230118443,-113.63549069373947]\n[47.22201164144353,-116.74259371362048]\n[44.20644767441859,-115.41792441860464]\n[49.05376490066225,-120.51894701986755]\n[47.19310087173103,-120.14485180572849]\n[44.20463568773233,-118.73157249070631]\n[46.052125,-118.73929761904759]\n[46.46156896551724,-110.87455172413796]\n[47.56744029850744,-110.21042537313429]\n[46.125213461538436,-116.05147884615387]\n[47.99982078313257,-120.22797289156627]\n[45.48587570621469,-117.70833898305082]\n[43.97910294117648,-117.29735294117646]\n[46.140986175115195,-123.47645622119815]\n[42.88455102040817,-112.8138857142857]\n[44.869622195985826,-121.15737190082645]\n[44.808635179153065,-111.84700651465802]\n[44.241822580645156,-120.60313548387101]\n[47.136439716312054,-114.43295390070921]\n[44.58817561983467,-110.4783099173554]\n[47.0657030927835,-117.13560206185569]\n[48.97412318840581,-116.18796859903384]\n[43.50731858407079,-117.11064159292035]\n[42.84999497487438,-121.73672361809044]\n[45.270273356401326,-110.77929065743939]\n[45.13745501955673,-115.259886571056]\n[42.1382846237731,-123.84929116684836]\n[44.58417830882352,-114.3680238970589]\n[47.04683809523809,-117.68902857142858]\n[46.50790157480312,-116.46165354330712]\n[48.993358620689655,-117.12691034482762]\n[46.86811904761903,-112.1304761904762]\n[42.28441,-120.02459999999999]\n[44.066554054054066,-114.74205405405402]\n[45.232585526315795,-114.70940350877189]\n[45.426522388059695,-116.51374626865673]\nfeatureCols: Array[String] = Array(latitude, longitude)\nassembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_7b92b7a90ee6\ndf2: org.apache.spark.sql.DataFrame = [latitude: double, longitude: double ... 1 more field]\ntrainingData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [latitude: double, longitude: double ... 1 more field]\ntestData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [latitude: double, longitude: double ... 1 more field]\nkmeans: org.apache.spark.ml.clustering.KMeans = kmeans_e75f8642b25a\nmodel: org.apache.spark.ml.clustering.KMeansModel = kmeans_e75f8642b25a\n"}]},"apps":[],"jobName":"paragraph_1550093831255_-1123936308","id":"20161030-041240_922666463","dateCreated":"2019-02-13T21:37:11+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:248","dateFinished":"2019-02-13T21:45:10+0000","dateStarted":"2019-02-13T21:45:08+0000","runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://8e7f2ab48c5c:4040/jobs/job?id=73","http://8e7f2ab48c5c:4040/jobs/job?id=74","http://8e7f2ab48c5c:4040/jobs/job?id=75","http://8e7f2ab48c5c:4040/jobs/job?id=76","http://8e7f2ab48c5c:4040/jobs/job?id=77","http://8e7f2ab48c5c:4040/jobs/job?id=78","http://8e7f2ab48c5c:4040/jobs/job?id=79","http://8e7f2ab48c5c:4040/jobs/job?id=80","http://8e7f2ab48c5c:4040/jobs/job?id=81","http://8e7f2ab48c5c:4040/jobs/job?id=82","http://8e7f2ab48c5c:4040/jobs/job?id=83"],"interpreterSettingId":"spark"}}},{"title":"Save the Model (optional)","text":"// Saved the k-means model to a file so it can be accessed later without creating it again from scratch:\nmodel.write.overwrite().save(\"/mapr/demo.mapr.com/user/mapr/data/save_fire_model-cascadia\")\n","user":"anonymous","dateUpdated":"2019-02-13T21:41:22+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"java.lang.RuntimeException: Unable to determine ResourceManager service address from Zookeeper at localhost.localdomain:5181\n  at org.apache.hadoop.yarn.client.MapRZKRMFinderUtils.mapRZkBasedRMFinder(MapRZKRMFinderUtils.java:121)\n  at org.apache.hadoop.yarn.client.MapRZKBasedRMAddressFinder.getRMAddress(MapRZKBasedRMAddressFinder.java:43)\n  at org.apache.hadoop.yarn.conf.HAUtil.getCurrentRMAddress(HAUtil.java:72)\n  at org.apache.hadoop.mapred.Master.getMasterAddress(Master.java:60)\n  at org.apache.hadoop.mapred.Master.getMasterPrincipal(Master.java:74)\n  at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodesInternal(TokenCache.java:114)\n  at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodesInternal(TokenCache.java:100)\n  at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodes(TokenCache.java:80)\n  at org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:126)\n  at org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.assertConf(SparkHadoopWriter.scala:283)\n  at org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:71)\n  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1096)\n  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094)\n  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n  at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1094)\n  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1067)\n  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1032)\n  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1032)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n  at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1032)\n  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:958)\n  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:958)\n  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:958)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n  at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:957)\n  at org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1493)\n  at org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1472)\n  at org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1472)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n  at org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1472)\n  at org.apache.spark.ml.util.DefaultParamsWriter$.saveMetadata(ReadWrite.scala:283)\n  at org.apache.spark.ml.clustering.KMeansModel$KMeansModelWriter.saveImpl(KMeans.scala:211)\n  at org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:103)\n  ... 65 elided\n"}]},"apps":[],"jobName":"paragraph_1550093831255_1602568983","id":"20171024-060732_541192335","dateCreated":"2019-02-13T21:37:11+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:249","dateFinished":"2019-02-13T21:41:23+0000","dateStarted":"2019-02-13T21:41:22+0000"},{"title":"Load the Model (optional)","text":"// If you've previously saved the model file, then you can load it like this:\nval model = KMeansModel.load(\"/mapr/demo.mapr.com/user/mapr/data/save_fire_model-cascadia\")","user":"anonymous","dateUpdated":"2019-02-13T21:42:19+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"java.lang.RuntimeException: Unable to determine ResourceManager service address from Zookeeper at localhost.localdomain:5181\n  at org.apache.hadoop.yarn.client.MapRZKRMFinderUtils.mapRZkBasedRMFinder(MapRZKRMFinderUtils.java:121)\n  at org.apache.hadoop.yarn.client.MapRZKBasedRMAddressFinder.getRMAddress(MapRZKBasedRMAddressFinder.java:43)\n  at org.apache.hadoop.yarn.conf.HAUtil.getCurrentRMAddress(HAUtil.java:72)\n  at org.apache.hadoop.mapred.Master.getMasterAddress(Master.java:60)\n  at org.apache.hadoop.mapred.Master.getMasterPrincipal(Master.java:74)\n  at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodesInternal(TokenCache.java:114)\n  at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodesInternal(TokenCache.java:100)\n  at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodes(TokenCache.java:80)\n  at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:206)\n  at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:317)\n  at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:204)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n  at scala.Option.getOrElse(Option.scala:121)\n  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n  at scala.Option.getOrElse(Option.scala:121)\n  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n  at org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1337)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n  at org.apache.spark.rdd.RDD.take(RDD.scala:1331)\n  at org.apache.spark.rdd.RDD$$anonfun$first$1.apply(RDD.scala:1372)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n  at org.apache.spark.rdd.RDD.first(RDD.scala:1371)\n  at org.apache.spark.ml.util.DefaultParamsReader$.loadMetadata(ReadWrite.scala:387)\n  at org.apache.spark.ml.clustering.KMeansModel$KMeansModelReader.load(KMeans.scala:231)\n  at org.apache.spark.ml.clustering.KMeansModel$KMeansModelReader.load(KMeans.scala:221)\n  at org.apache.spark.ml.util.MLReadable$class.load(ReadWrite.scala:223)\n  at org.apache.spark.ml.clustering.KMeansModel$.load(KMeans.scala:195)\n  ... 65 elided\n"}]},"apps":[],"jobName":"paragraph_1550093831256_923295361","id":"20181002-140101_870252359","dateCreated":"2019-02-13T21:37:11+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:250","dateFinished":"2019-02-13T21:42:20+0000","dateStarted":"2019-02-13T21:42:19+0000"},{"title":"Setup Map (save centroids)","text":"z.angularBind(\"centroid\", model.clusterCenters)","user":"anonymous","dateUpdated":"2019-02-13T21:42:31+0000","config":{"tableHide":true,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"fontSize":9,"title":true,"results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1550093831256_-651469192","id":"20161116-075433_1562509402","dateCreated":"2019-02-13T21:37:11+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:251","dateFinished":"2019-02-13T21:42:32+0000","dateStarted":"2019-02-13T21:42:31+0000"},{"title":"Setup Map (initialize angular)","text":"%angular\n<!-- Avoid constantly editing JS and list the Angular vars you want exposed in an HTML attribute: -->\n<div id=\"dummy\" vars=\"centroid\"></div>\n<script type=\"text/javascript\">\n  //Given an element in the note & list of values to fetch from Spark\n  //window.angularVars.myVal will be current value of backend Spark val of same name\n  function hoist(element){\n    var varNames = element.attr('vars').split(',');\n    window.angularVars = {};\n    var scope = angular.element(element.parent('.ng-scope')).scope().compiledScope;\n    $.each(varNames, function(i, v){\n      window[v+'-watcher'] = scope.$watch(v, function(newVal, oldVal){\n        window.angularVars[v] = newVal;\n      });\n    });\n  }\n  hoist($('#dummy'));\n</script>","user":"anonymous","dateUpdated":"2019-02-13T21:42:33+0000","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/undefined","editorHide":true,"fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"ANGULAR","data":"<!-- Avoid constantly editing JS and list the Angular vars you want exposed in an HTML attribute: -->\n<div id=\"dummy\" vars=\"centroid\"></div>\n<script type=\"text/javascript\">\n  //Given an element in the note & list of values to fetch from Spark\n  //window.angularVars.myVal will be current value of backend Spark val of same name\n  function hoist(element){\n    var varNames = element.attr('vars').split(',');\n    window.angularVars = {};\n    var scope = angular.element(element.parent('.ng-scope')).scope().compiledScope;\n    $.each(varNames, function(i, v){\n      window[v+'-watcher'] = scope.$watch(v, function(newVal, oldVal){\n        window.angularVars[v] = newVal;\n      });\n    });\n  }\n  hoist($('#dummy'));\n</script>"}]},"apps":[],"jobName":"paragraph_1550093831256_-1821472845","id":"20171024-055715_1739605740","dateCreated":"2019-02-13T21:37:11+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:252","dateFinished":"2019-02-13T21:42:35+0000","dateStarted":"2019-02-13T21:42:33+0000"},{"title":"Where would you stage firefighting equipment?","text":"%angular\n<div id=\"map\" style=\"height:500px;width:100%;\"></div>\n<script type=\"text/javascript\">\nfunction circle(pos, color, map){\n    var circle = new google.maps.Circle({\n        strokeColor: color,\n        strokeOpacity: 0.8,\n        strokeWeight: 1,\n        fillColor: color,\n        fillOpacity: 0.3,\n        map: map,\n        center: pos,\n        radius: 40000   //default: 6.5\n    });\n}\n\n\nfunction initMap() {\n    var centroid = window.angularVars.centroid;\n    var POIs = {};\n    var USA = {lat: 39.8282, lng: -98.5795};\n    var Cascadia =  {lat: 45.5, lng: -115};\n    var map = new google.maps.Map(document.getElementById('map'), {zoom: 6, center: Cascadia });\n    //  Draw circles for each Centroid\n    $.each(centroid, function(i, v){\n         POIs[v.values[0]] = v.values;\n         var pos = {lat: parseFloat(v.values[0]), lng: parseFloat(v.values[1]) };\n         var color = '#FF0000'\n         circle(pos, color, map);\n    });\n}\n\n//Only load GMaps once\nif (typeof google === 'object' && typeof google.maps === 'object') initMap();\nelse {\n    var script = document.createElement(\"script\");\n    script.type = \"text/javascript\";\n    //Replace with your API key\n    var apiKey = 'AIzaSyCvFVSdCznu_b-pgSAFN9gqVGiFXngAvzk';\n    script.src = \"https://maps.googleapis.com/maps/api/js?key=\"+apiKey+\"&callback=initMap\";\n    document.body.appendChild(script);\n}\n</script>","user":"anonymous","dateUpdated":"2019-02-13T21:42:36+0000","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/undefined","editorHide":true,"fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"ANGULAR","data":"<div id=\"map\" style=\"height:500px;width:100%;\"></div>\n<script type=\"text/javascript\">\nfunction circle(pos, color, map){\n    var circle = new google.maps.Circle({\n        strokeColor: color,\n        strokeOpacity: 0.8,\n        strokeWeight: 1,\n        fillColor: color,\n        fillOpacity: 0.3,\n        map: map,\n        center: pos,\n        radius: 40000   //default: 6.5\n    });\n}\n\n\nfunction initMap() {\n    var centroid = window.angularVars.centroid;\n    var POIs = {};\n    var USA = {lat: 39.8282, lng: -98.5795};\n    var Cascadia =  {lat: 45.5, lng: -115};\n    var map = new google.maps.Map(document.getElementById('map'), {zoom: 6, center: Cascadia });\n    //  Draw circles for each Centroid\n    $.each(centroid, function(i, v){\n         POIs[v.values[0]] = v.values;\n         var pos = {lat: parseFloat(v.values[0]), lng: parseFloat(v.values[1]) };\n         var color = '#FF0000'\n         circle(pos, color, map);\n    });\n}\n\n//Only load GMaps once\nif (typeof google === 'object' && typeof google.maps === 'object') initMap();\nelse {\n    var script = document.createElement(\"script\");\n    script.type = \"text/javascript\";\n    //Replace with your API key\n    var apiKey = 'AIzaSyCvFVSdCznu_b-pgSAFN9gqVGiFXngAvzk';\n    script.src = \"https://maps.googleapis.com/maps/api/js?key=\"+apiKey+\"&callback=initMap\";\n    document.body.appendChild(script);\n}\n</script>"}]},"apps":[],"jobName":"paragraph_1550093831257_1191185315","id":"20171024-055723_1332952506","dateCreated":"2019-02-13T21:37:11+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:253","dateFinished":"2019-02-13T21:42:36+0000","dateStarted":"2019-02-13T21:42:36+0000"},{"text":"%md # Operationalizing the model\n***Which staging area should respond when a new forest fire starts?***\nWe can use our previously saved model to answer that question. The following section shows how the model we built above can be used to answer that question and how it can be applied to a live feed of newly detected fires for rapid fire response.\n","user":"anonymous","dateUpdated":"2019-02-13T21:37:11+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Operationalizing the model</h1>\n<p><strong><em>Which staging area should respond when a new forest fire starts?</em></strong><br/>We can use our previously saved model to answer that question. The following section shows how the model we built above can be used to answer that question and how it can be applied to a live feed of newly detected fires for rapid fire response.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1550093831257_-1312984198","id":"20171110-050323_189619447","dateCreated":"2019-02-13T21:37:11+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:254"},{"title":"Which fire station (centroid) should respond to a new fire?","text":"val featureCols = Array(\"lat\", \"lon\")\nval assembler = new VectorAssembler().setInputCols(featureCols).setOutputCol(\"features\")\nval fire_location = Seq((42.3,-112.2 )).toDF(\"lat\", \"lon\")\nval df3 = assembler.transform(fire_location)\nval categories = model.transform(df3)\nval centroid_id = categories.select(\"prediction\").rdd.map(r => r(0)).collect()(0).asInstanceOf[Int]\nval centroid_coordinate = model.clusterCenters(centroid_id)","user":"anonymous","dateUpdated":"2019-02-13T21:42:44+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"featureCols: Array[String] = Array(lat, lon)\nassembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_ed4889e1f96a\nfire_location: org.apache.spark.sql.DataFrame = [lat: double, lon: double]\ndf3: org.apache.spark.sql.DataFrame = [lat: double, lon: double ... 1 more field]\ncategories: org.apache.spark.sql.DataFrame = [lat: double, lon: double ... 2 more fields]\ncentroid_id: Int = 6\ncentroid_coordinate: org.apache.spark.ml.linalg.Vector = [42.511442224153,-111.79813987836673]\n"}]},"apps":[],"jobName":"paragraph_1550093831258_-1985323542","id":"20171109-002326_1996678963","dateCreated":"2019-02-13T21:37:11+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:255","dateFinished":"2019-02-13T21:42:45+0000","dateStarted":"2019-02-13T21:42:44+0000","runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://8e7f2ab48c5c:4040/jobs/job?id=35"],"interpreterSettingId":"spark"}}},{"text":"println(\"%html <h1>Notify fire station #\" + centroid_id + \" at location \" + centroid_coordinate)","user":"anonymous","dateUpdated":"2019-02-13T21:42:48+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Notify fire station #6 at location [42.511442224153,-111.79813987836673]\n"}]},"apps":[],"jobName":"paragraph_1550093831258_496320508","id":"20190213-194349_2112090717","dateCreated":"2019-02-13T21:37:11+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:256","dateFinished":"2019-02-13T21:42:48+0000","dateStarted":"2019-02-13T21:42:48+0000"},{"text":"%md Now that we can identify which fire station should respond to a fire, \nlets identify which fire stations should respond to all the currently active fires posted by USFS.","user":"anonymous","dateUpdated":"2019-02-13T21:37:11+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Now that we can identify which fire station should respond to a fire,<br/>lets identify which fire stations should respond to all the currently active fires posted by USFS.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1550093831258_814537180","id":"20171110-054814_331474225","dateCreated":"2019-02-13T21:37:11+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:257"},{"title":"Initialize web scraper","text":"%pyspark \nimport requests\nimport pandas as pd\nfrom bs4 import BeautifulSoup\n\nclass HTMLTableParser:   \n    def parse_url(self, url):\n        response = requests.get(url)\n        soup = BeautifulSoup(response.text, 'lxml')\n        return [(table,self.parse_html_table(table))\\\n                for table in soup.find_all('table')]  \n    def parse_html_table(self, table):\n        n_columns = 0\n        n_rows=0\n        column_names = []\n        # Find number of rows and columns\n        # we also find the column titles if we can\n        for row in table.find_all('tr'):            \n            # Determine the number of rows in the table\n            td_tags = row.find_all('td')\n            if len(td_tags) > 0:\n                n_rows+=1\n                if n_columns == 0:\n                    # Set the number of columns for our table\n                    n_columns = len(td_tags)\n            # Handle column names if we find them\n            th_tags = row.find_all('th') \n            if len(th_tags) > 0 and len(column_names) == 0:\n                for th in th_tags:\n                    column_names.append(th.get_text())\n        # Safeguard on Column Titles\n        if len(column_names) > 0 and len(column_names) != n_columns:\n            raise Exception(\"Column titles do not match the number of columns\")\n        columns = column_names if len(column_names) > 0 else range(0,n_columns)\n        df = pd.DataFrame(columns = columns,\n                          index= range(0,n_rows))\n        row_marker = 0\n        for row in table.find_all('tr'):\n            column_marker = 0\n            columns = row.find_all('td')\n            for column in columns:\n                df.iat[row_marker,column_marker] = column.get_text()\n                column_marker += 1\n            if len(columns) > 0:\n                row_marker += 1\n        # Convert to float if possible\n        for col in df:\n            try:\n                df[col] = df[col].astype(float)\n            except ValueError:\n                pass \n        return df","user":"anonymous","dateUpdated":"2019-02-13T21:42:58+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","editorHide":true,"fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1550093831259_2032134994","id":"20171109-002346_1337732733","dateCreated":"2019-02-13T21:37:11+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:258","dateFinished":"2019-02-13T21:42:59+0000","dateStarted":"2019-02-13T21:42:58+0000"},{"title":"Scrape real-time fire detections from web","text":"%pyspark \nurl=\"https://fsapps.nwcg.gov/afm/current.php?op=table&sensor=modis\"\nhp = HTMLTableParser()\ndf = hp.parse_url(url)[4][1] \n# rename the columns according to what's in the first row\ndf.columns = df.iloc[0]\ndf = df.reindex(df.index.drop(0))\nspark_df = sqlContext.createDataFrame(df)\nspark_df.registerTempTable(\"active_fires\") # Save as table\nspark_df.printSchema()","user":"anonymous","dateUpdated":"2019-02-13T21:42:59+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{"0":{"graph":{"mode":"table","height":300,"optionOpen":true,"setting":{"multiBarChart":{},"pieChart":{}},"commonSetting":{},"keys":[{"name":"10","index":10,"aggr":"sum"}],"groups":[{"name":"8","index":8,"aggr":"sum"}],"values":[{"name":"10","index":10,"aggr":"sum"}]},"helium":{}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- Date (GMT): string (nullable = true)\n |-- Time (GMT): string (nullable = true)\n |-- Satellite: string (nullable = true)\n |-- Longitude(WGS84): string (nullable = true)\n |-- Latitude(WGS84): string (nullable = true)\n |-- Agency: string (nullable = true)\n |-- Admin Unit: string (nullable = true)\n |-- Land Cover: string (nullable = true)\n |-- Fire Danger: string (nullable = true)\n |-- County: string (nullable = true)\n |-- State: string (nullable = true)\n |-- Nearest Town: string (nullable = true)\n |-- Distance to Town(mi): string (nullable = true)\n\n"}]},"apps":[],"jobName":"paragraph_1550093831259_1831898007","id":"20171109-002358_1073971632","dateCreated":"2019-02-13T21:37:11+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:259","dateFinished":"2019-02-13T21:43:03+0000","dateStarted":"2019-02-13T21:43:00+0000"},{"title":"","text":"%pyspark \nprint(\"%html <h1> There are \" + str(len(df.index)) + \" fires currently active.</h1>\")","user":"anonymous","dateUpdated":"2019-02-13T21:43:03+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1> There are 102 fires currently active.</h1>\n"}]},"apps":[],"jobName":"paragraph_1550093831260_1083526147","id":"20171109-002629_2093508410","dateCreated":"2019-02-13T21:37:11+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:260","dateFinished":"2019-02-13T21:43:03+0000","dateStarted":"2019-02-13T21:43:03+0000"},{"title":"Active Fires by State","text":"%sql \nselect count(`Land Cover`), state from active_fires  where `state` not like '%STATE%' group by state order by count(`Land Cover`) desc limit 8","user":"anonymous","dateUpdated":"2019-02-13T21:43:11+0000","config":{"editorSetting":{"language":"sql","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/sql","fontSize":9,"title":true,"results":{"0":{"graph":{"mode":"multiBarChart","height":300,"optionOpen":false,"setting":{"multiBarChart":{"rotate":{"degree":"-45"},"xLabelStatus":"default"},"table":{"tableGridState":{},"tableColumnTypeState":{"names":{"count(Land Cover)":"string","state":"string"},"updated":false},"tableOptionSpecHash":"[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]","tableOptionValue":{"useFilter":false,"showPagination":false,"showAggregationFooter":false},"updated":false,"initialized":false},"stackedAreaChart":{"rotate":{"degree":"-45"},"xLabelStatus":"default"}},"commonSetting":{},"keys":[{"name":"state","index":1,"aggr":"sum"}],"groups":[],"values":[{"name":"count(Land Cover)","index":0,"aggr":"sum"}]},"helium":{}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"count(Land Cover)\tstate\n51\t'Texas'\n20\t'Alabama'\n14\t'Arkansas'\n6\t'Georgia'\n4\t'Oklahoma'\n3\t'Florida'\n2\t'Louisiana'\n2\t'Mississippi'\n"},{"type":"TEXT","data":""}]},"apps":[],"jobName":"paragraph_1550093831260_2108397541","id":"20171110-063717_362478632","dateCreated":"2019-02-13T21:37:11+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:261","dateFinished":"2019-02-13T21:43:11+0000","dateStarted":"2019-02-13T21:43:09+0000","runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://8e7f2ab48c5c:4040/jobs/job?id=36"],"interpreterSettingId":"spark"}}},{"title":"Cumulative Fire Danger","text":"%sql\nselect `Fire Danger`, `Land Cover` from active_fires where `Fire Danger` not like '%N-A%'\n","user":"anonymous","dateUpdated":"2019-02-13T21:43:16+0000","config":{"editorSetting":{"language":"sql","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/sql","fontSize":9,"title":true,"results":{"0":{"graph":{"mode":"pieChart","height":300,"optionOpen":false,"setting":{"table":{"tableGridState":{},"tableColumnTypeState":{"names":{"Fire Danger":"string","Land Cover":"string","State":"string"},"updated":false},"tableOptionSpecHash":"[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]","tableOptionValue":{"useFilter":false,"showPagination":false,"showAggregationFooter":false},"updated":false,"initialized":false},"pieChart":{},"multiBarChart":{"rotate":{"degree":"-45"},"xLabelStatus":"default"},"stackedAreaChart":{"rotate":{"degree":"-45"},"xLabelStatus":"default"}},"commonSetting":{},"keys":[{"name":"Fire Danger","index":0,"aggr":"sum"}],"groups":[],"values":[{"name":"Land Cover","index":1,"aggr":"count"}]},"helium":{}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"Fire Danger\tLand Cover\n'Low'\t'Grasslands'\n'Low'\t'Grasslands'\n'Low'\t'Woody Savannas'\n'Low'\t'Cropland/Natural Vegetation Mosaic'\n'Low'\t'Savannas'\n'Low'\t'Grasslands'\n'Low'\t'Croplands'\n'Low'\t'Cropland/Natural Vegetation Mosaic'\n'Low'\t'Cropland/Natural Vegetation Mosaic'\n'Low'\t'Croplands'\n'Low'\t'Croplands'\n'Low'\t'Grasslands'\n'Moderate'\t'Urban and Built-Up'\n'Low'\t'Croplands'\n'Low'\t'Croplands'\n'Low'\t'Grasslands'\n'Low'\t'Woody Savannas'\n'Moderate'\t'Croplands'\n'Low'\t'Cropland/Natural Vegetation Mosaic'\n'Low'\t'Grasslands'\n'Low'\t'Grasslands'\n'Low'\t'Croplands'\n'Low'\t'Croplands'\n'Low'\t'Croplands'\n'Low'\t'Croplands'\n'Low'\t'Croplands'\n'Low'\t'Open Shrublands'\n'Low'\t'Grasslands'\n'Low'\t'Cropland/Natural Vegetation Mosaic'\n'Low'\t'Cropland/Natural Vegetation Mosaic'\n'Low'\t'Grasslands'\n'Low'\t'Cropland/Natural Vegetation Mosaic'\n'Low'\t'Savannas'\n'Low'\t'Cropland/Natural Vegetation Mosaic'\n'Low'\t'Croplands'\n'Moderate'\t'Cropland/Natural Vegetation Mosaic'\n'Low'\t'Woody Savannas'\n'Low'\t'Woody Savannas'\n'Moderate'\t'N-A'\n'Moderate'\t'Croplands'\n'Moderate'\t'Woody Savannas'\n'Moderate'\t'Woody Savannas'\n'Low'\t'Cropland/Natural Vegetation Mosaic'\n'Low'\t'Croplands'\n'Low'\t'Croplands'\n'Low'\t'Croplands'\n'Low'\t'Cropland/Natural Vegetation Mosaic'\n'Low'\t'Cropland/Natural Vegetation Mosaic'\n'Low'\t'Evergreen Broadleaf Forest'\n'Low'\t'Cropland/Natural Vegetation Mosaic'\n'Low'\t'Cropland/Natural Vegetation Mosaic'\n'Moderate'\t'Croplands'\n'Low'\t'Deciduous Broadleaf Forest'\n'Low'\t'Croplands'\n'Low'\t'Mixed Forests'\n'Low'\t'Croplands'\n'Moderate'\t'Cropland/Natural Vegetation Mosaic'\n'Low'\t'Grasslands'\n'Moderate'\t'Grasslands'\n'Low'\t'Grasslands'\n'Moderate'\t'Woody Savannas'\n'Moderate'\t'Croplands'\n'Low'\t'Evergreen Broadleaf Forest'\n'Low'\t'Croplands'\n'Low'\t'Croplands'\n'Low'\t'Mixed Forests'\n'Moderate'\t'Cropland/Natural Vegetation Mosaic'\n'Low'\t'Grasslands'\n'Moderate'\t'Cropland/Natural Vegetation Mosaic'\n'Low'\t'Woody Savannas'\n'Moderate'\t'Cropland/Natural Vegetation Mosaic'\n'Low'\t'Cropland/Natural Vegetation Mosaic'\n'Low'\t'Woody Savannas'\n'Low'\t'Woody Savannas'\n'Low'\t'Woody Savannas'\n'Low'\t'Croplands'\n'Low'\t'Woody Savannas'\n'Low'\t'Evergreen Needleleaf Forest'\n'Low'\t'Mixed Forests'\n'Low'\t'Woody Savannas'\n'Low'\t'Croplands'\n'Low'\t'Woody Savannas'\n'Low'\t'Grasslands'\n'Low'\t'Cropland/Natural Vegetation Mosaic'\n'Low'\t'Croplands'\n'Low'\t'Savannas'\n'Low'\t'Croplands'\n'Low'\t'Grasslands'\n'Low'\t'Woody Savannas'\n'Low'\t'Cropland/Natural Vegetation Mosaic'\n'Moderate'\t'Woody Savannas'\n'Low'\t'Cropland/Natural Vegetation Mosaic'\n'Low'\t'Cropland/Natural Vegetation Mosaic'\n'Low'\t'Savannas'\n'Low'\t'Woody Savannas'\n'Low'\t'Woody Savannas'\n'Low'\t'Grasslands'\n"},{"type":"TEXT","data":""}]},"apps":[],"jobName":"paragraph_1550093831261_65697950","id":"20171110-064617_1839772878","dateCreated":"2019-02-13T21:37:11+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:262","dateFinished":"2019-02-13T21:43:17+0000","dateStarted":"2019-02-13T21:43:17+0000","runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://8e7f2ab48c5c:4040/jobs/job?id=37","http://8e7f2ab48c5c:4040/jobs/job?id=38","http://8e7f2ab48c5c:4040/jobs/job?id=39"],"interpreterSettingId":"spark"}}},{"title":"Initialize streams","text":"%sh\n# start streams\nmaprcli stream create -path /user/mapr/ml_input -produceperm p -consumeperm p -topicperm p -ttl 604800\nmaprcli stream topic create -path /user/mapr/ml_input -topic requester001\n# ttl 604800 is 1 week\nmaprcli stream create -path /user/mapr/ml_output -produceperm p -consumeperm p -topicperm p -ttl 604800\nmaprcli stream topic create -path /user/mapr/ml_output -topic kmeans001","user":"anonymous","dateUpdated":"2019-02-13T21:37:47+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1550093831261_-571082970","id":"20171110-072208_814620619","dateCreated":"2019-02-13T21:37:11+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:263","dateFinished":"2019-02-13T21:37:58+0000","dateStarted":"2019-02-13T21:37:47+0000"},{"title":"Start the Spark Streaming notification task","text":"%sh \n# run this in a terminal - doesn't work well in Zeppelin because it ties up the spark context while listening on the stream\n/opt/mapr/spark/spark-2.3.1/bin/spark-submit --class com.sparkkafka.fire.SparkKafkaConsumerProducer --master local[2] /root/mapr-sparkml-streaming-fires-1.0-jar-with-dependencies.jar /mapr/demo.mapr.com/user/mapr/data/save_fire_model-cascadia  /user/mapr/ml_input:requester001 /user/mapr/ml_output:kmeans001  \n","user":"anonymous","dateUpdated":"2019-02-13T21:38:06+0000","config":{"tableHide":false,"editorSetting":{"language":"sh","editOnDblClick":false,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"title":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Warning: Unable to determine $DRILL_HOME\n"}]},"apps":[],"jobName":"paragraph_1550093831261_-243865837","id":"20171110-003135_94995843","dateCreated":"2019-02-13T21:37:11+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:264"},{"title":"Start websocket interface for streams","text":"%sh\n# run this in a terminal - doesn't work well in Zeppelin\nwebsocketd --port=3433 --dir=. --devconsole &\n","user":"anonymous","dateUpdated":"2019-02-13T21:37:11+0000","config":{"tableHide":true,"editorSetting":{"language":"sh","editOnDblClick":false,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"title":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1550093831262_-1264128501","id":"20171110-073509_233231405","dateCreated":"2019-02-13T21:37:11+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:265"},{"title":"Identify which fire stations (centroids) should respond to active fires","text":"%pyspark \nimport websocket\nwebsocket.enableTrace(False)\nws = websocket.create_connection(\"ws://10.0.0.101:3433/ml_input_stream.sh\")\nfor index, row in df.iterrows():\n    ws.send(row['Latitude(WGS84)']+\",\"+row['Longitude(WGS84)'])\n    print(row['Latitude(WGS84)']+\",\"+row['Longitude(WGS84)'])\n    ws.recv()\nws.close()","user":"anonymous","dateUpdated":"2019-02-13T21:37:11+0000","config":{"tableHide":false,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"java.net.ConnectException: Connection refused\n\tat java.net.PlainSocketImpl.socketConnect(Native Method)\n\tat java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)\n\tat java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)\n\tat java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)\n\tat java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n\tat java.net.Socket.connect(Socket.java:589)\n\tat org.apache.thrift.transport.TSocket.open(TSocket.java:182)\n\tat org.apache.zeppelin.interpreter.remote.ClientFactory.create(ClientFactory.java:51)\n\tat org.apache.zeppelin.interpreter.remote.ClientFactory.create(ClientFactory.java:37)\n\tat org.apache.commons.pool2.BasePooledObjectFactory.makeObject(BasePooledObjectFactory.java:60)\n\tat org.apache.commons.pool2.impl.GenericObjectPool.create(GenericObjectPool.java:861)\n\tat org.apache.commons.pool2.impl.GenericObjectPool.borrowObject(GenericObjectPool.java:435)\n\tat org.apache.commons.pool2.impl.GenericObjectPool.borrowObject(GenericObjectPool.java:363)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterProcess.getClient(RemoteInterpreterProcess.java:62)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterProcess.callRemoteFunction(RemoteInterpreterProcess.java:133)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreter.internal_create(RemoteInterpreter.java:165)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreter.open(RemoteInterpreter.java:132)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreter.getFormType(RemoteInterpreter.java:299)\n\tat org.apache.zeppelin.notebook.Paragraph.jobRun(Paragraph.java:407)\n\tat org.apache.zeppelin.scheduler.Job.run(Job.java:188)\n\tat org.apache.zeppelin.scheduler.RemoteScheduler$JobRunner.run(RemoteScheduler.java:307)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n"}]},"apps":[],"jobName":"paragraph_1550093831262_-1820614749","id":"20171110-001243_1476311896","dateCreated":"2019-02-13T21:37:11+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:266"},{"text":"%md \n### Click [here](http://10.0.0.101:3433/ml_input_stream.sh) to watch the fire notifications stream (ML input).","user":"anonymous","dateUpdated":"2019-02-13T21:37:11+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Click <a href=\"http://10.0.0.101:3433/ml_input_stream.sh\">here</a> to watch the fire notifications stream (ML input).</h3>\n</div>"}]},"apps":[],"jobName":"paragraph_1550093831262_-1471425979","id":"20171110-002907_888967422","dateCreated":"2019-02-13T21:37:11+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:267"},{"text":"%md \n### Click [here](http://10.0.0.101:3433/ml_output_stream.sh) to watch the fire station alert stream (ML output).","user":"anonymous","dateUpdated":"2019-02-13T21:37:11+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Click <a href=\"http://10.0.0.101:3433/ml_output_stream.sh\">here</a> to watch the fire station alert stream (ML output).</h3>\n</div>"}]},"apps":[],"jobName":"paragraph_1550093831263_1026183003","id":"20171110-045926_1106373116","dateCreated":"2019-02-13T21:37:11+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:268"},{"title":"Backup training data and model files","text":"%sh\nmaprcli volume snapshot remove -volume mapr_home -snapshotname USFS_Experiment \nmaprcli volume snapshot create -cluster my.cluster.com -snapshotname USFS_Experiment -volume mapr_home\nls -la /mapr/my.cluster.com/user/mapr/.snapshot/","user":"anonymous","dateUpdated":"2019-02-13T21:37:11+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1550093831263_2133196201","id":"20171109-002734_2068244197","dateCreated":"2019-02-13T21:37:11+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:269"},{"text":"\n","user":"anonymous","dateUpdated":"2019-02-13T21:37:11+0000","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1550093831264_1058875361","id":"20171024-060331_621482076","dateCreated":"2019-02-13T21:37:11+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:270"}],"name":"Forest Fire Prediction","id":"2E3TWXHDJ","noteParams":{},"noteForms":{},"angularObjects":{},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}